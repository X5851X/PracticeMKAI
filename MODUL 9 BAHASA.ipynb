{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9368084d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Risvandiani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Risvandiani\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03938eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open('intentsindo.json').read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffbad875",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore = ['?']\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w) \n",
    "        documents.append((w, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10485af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 documents\n",
      "30 classes ['Identity', 'activity', 'age', 'appreciate', 'contact', 'covid19', 'cricket', 'datetime', 'exclaim', 'goodbye', 'google', 'greeting', 'greetreply', 'haha', 'inspire', 'insult', 'jokes', 'karan', 'news', 'nicetty', 'no', 'options', 'programmer', 'riddle', 'song', 'suggest', 'thanks', 'timer', 'weather', 'whatsup']\n",
      "131 unique stemmed words [',', '10', '19', 'ad', 'aku', 'and', 'ap', 'apakah', 'at', 'ata', 'bag', 'bagaiman', 'baik', 'baik-baik', 'bantu', 'bantuanny', 'banyak', 'bby', 'berap', 'berbicar', 'bergun', 'berit', 'bertemu', 'bias', 'bis', 'bodoh', 'buat', 'buruk', 'car', 'ceritak', 'covid', 'cricket', 'cuac', 'deng', 'di', 'dia', 'diam', 'dibu', 'ditawark', 'dukung', 'googl', 'hah', 'halo', 'har', 'heb', 'hei', 'hilanglah', 'hingg', 'hol', 'hubung', 'idiot', 'in', 'ind', 'inspiras', 'internet', 'itu', 'jam', 'jok', 'jump', 'kab', 'kabarmu', 'kamu', 'kap', 'kar', 'kasih', 'lag', 'lagu', 'lagu-lagu', 'lakuk', 'lmao', 'lol', 'luar', 'lucu', 'malik', 'membantu', 'membuatmu', 'memotivas', 'memprogrammu', 'menginspiras', 'menyenangk', 'merancangmu', 'namast', 'nant', 'ok', 'padaku', 'panasny', 'pembu', 'peng', 'pengembang', 'percakap', 'pertand', 'pertanya', 'pop', 'program', 'rofl', 'saat', 'saj', 'sampa', 'san', 'sang', 'sar', 'say', 'seberap', 'sedang', 'sediak', 'sekal', 'sekarang', 'selam', 'sepuluh', 'siang', 'siap', 'skor', 'suhu', 'tahu', 'tang', 'tanyak', 'teka-teki', 'terata', 'terbaik', 'terbaru', 'terim', 'tertaw', 'tidak', 'ting', 'umurmu', 'usiamu', 'usulk', 'waktu', 'ya', 'yang', 'yo']\n"
     ]
    }
   ],
   "source": [
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
    "words = sorted(list(set(words)))\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd037adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "        \n",
    "    training.append([bag, output_row])\n",
    "\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "\n",
    "train_x = list(training[: ,0])\n",
    "train_y = list(training[: ,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ed0d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 14999  | total loss: \u001b[1m\u001b[32m0.03129\u001b[0m\u001b[0m | time: 0.074s\n",
      "| Adam | epoch: 1000 | loss: 0.03129 - acc: 0.9834 -- iter: 112/113\n",
      "Training Step: 15000  | total loss: \u001b[1m\u001b[32m0.02816\u001b[0m\u001b[0m | time: 0.086s\n",
      "| Adam | epoch: 1000 | loss: 0.02816 - acc: 0.9850 -- iter: 113/113\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Risvandiani\\Kecerdasan Buatan (AI)\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ba9e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump({'words': words, 'classes': classes, 'train_x': train_x, 'train_y': train_y}, open(\"training_data\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5593576",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"training_data\", \"rb\"))\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9059abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Risvandiani\\Kecerdasan Buatan (AI)\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "model.load('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc6be7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence, words, show_details=False):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "        return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a5f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_THRESHOLD = 0.30\n",
    "def classify(sentence):\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    return return_list\n",
    "def response(sentence, userID='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    if results:\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    return print(random.choice(i['responses']))\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ddb774e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tekan 0 jika kamu tidak mau untuk mengobrol dengan chatbot kami.\n",
      "halo\n",
      "Halo\n",
      "None\n",
      "apa kabar?\n",
      "Semuanya baik. Bagaimana dengan Anda?\n",
      "None\n",
      "saya baik\n",
      "Sampai jumpa!\n",
      "None\n",
      "oke\n",
      "Ya!\n",
      "None\n",
      "aku lapar\n",
      "Sampai jumpa!\n",
      "None\n",
      "baik\n",
      "Senang mendengarnya!\n",
      "None\n",
      "apa kabar?\n",
      "Semuanya baik. Bagaimana dengan Anda?\n",
      "None\n",
      "senang\n",
      "Ya!\n",
      "None\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tekan 0 jika kamu tidak mau untuk mengobrol dengan chatbot kami.\")\n",
    "while True:\n",
    "    message = input(\"\")\n",
    "    if message == \"0\":\n",
    "        break\n",
    "    result = response(message)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816b672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
